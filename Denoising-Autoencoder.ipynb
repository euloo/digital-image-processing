{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains a denoising autoencoder on MNIST dataset.\n",
    "\n",
    "Denoising is one of the classic applications of autoencoders.\n",
    "The denoising process removes unwanted noise that corrupted the\n",
    "true signal.\n",
    "Noise + Data ---> Denoising Autoencoder ---> Data\n",
    "Given a training dataset of corrupted data as input and\n",
    "true signal as output, a denoising autoencoder can recover the\n",
    "hidden structure to generate clean data.\n",
    "This example has modular design. The encoder, decoder and autoencoder\n",
    "are 3 models that share weights. For example, after training the\n",
    "autoencoder, the encoder can be used to  generate latent vectors\n",
    "of input data for low-dim visualization like PCA or TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate corrupted MNIST images by adding noise with normal dist\n",
    "# centered at 0.5 and std=0.5\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n",
    "x_train_noisy = x_train + noise\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n",
    "x_test_noisy = x_test + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "latent_dim = 16\n",
    "# Encoder/Decoder number of CNN layers and filters per layer\n",
    "layer_filters = [32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\usr\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Build the Autoencoder Model\n",
    "# First build the Encoder Model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = inputs\n",
    "# Stack of Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use MaxPooling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters:\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=2,\n",
    "               activation='relu',\n",
    "               padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape info needed to build Decoder Model\n",
    "shape = K.int_shape(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the latent vector\n",
    "x = Flatten()(x)\n",
    "latent = Dense(latent_dim, name='latent_vector')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "latent_vector (Dense)        (None, 16)                50192     \n",
      "=================================================================\n",
      "Total params: 69,008\n",
      "Trainable params: 69,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Encoder Model\n",
    "encoder = Model(inputs, latent, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Decoder Model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack of Transposed Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use UpSampling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters[::-1]:\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=kernel_size,\n",
    "                        strides=2,\n",
    "                        activation='relu',\n",
    "                        padding='same')(x)\n",
    "\n",
    "x = Conv2DTranspose(filters=1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same')(x)\n",
    "\n",
    "outputs = Activation('sigmoid', name='decoder_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3136)              53312     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "_________________________________________________________________\n",
      "decoder_output (Activation)  (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 108,993\n",
      "Trainable params: 108,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Decoder Model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 16)                69008     \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 28, 28, 1)         108993    \n",
      "=================================================================\n",
      "Total params: 178,001\n",
      "Trainable params: 178,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder = Encoder + Decoder\n",
    "# Instantiate Autoencoder Model\n",
    "autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\usr\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.0603 - val_loss: 0.0341\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0270 - val_loss: 0.0227\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.0215 - val_loss: 0.0201\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.0196 - val_loss: 0.0187\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0185 - val_loss: 0.0180\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.0178 - val_loss: 0.0176\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.0174 - val_loss: 0.0173\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.0170 - val_loss: 0.0169\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.0164 - val_loss: 0.0166\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0162 - val_loss: 0.0165\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0159 - val_loss: 0.0163\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0158 - val_loss: 0.0163\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0156 - val_loss: 0.0162\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0155 - val_loss: 0.0161\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0153 - val_loss: 0.0160\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0152 - val_loss: 0.0160\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0151 - val_loss: 0.0159\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0150 - val_loss: 0.0160\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0149 - val_loss: 0.0159\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0148 - val_loss: 0.0157\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0148 - val_loss: 0.0158\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0147 - val_loss: 0.0156\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0146 - val_loss: 0.0159\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0145 - val_loss: 0.0157\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0145 - val_loss: 0.0158\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0144 - val_loss: 0.0157\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0144 - val_loss: 0.0156\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0143 - val_loss: 0.0156\n",
      "Epoch 30/30\n",
      "11392/60000 [====>.........................] - ETA: 5s - loss: 0.0141"
     ]
    }
   ],
   "source": [
    "# Train the autoencoder\n",
    "autoencoder.fit(x_train_noisy,\n",
    "                x_train,\n",
    "                validation_data=(x_test_noisy, x_test),\n",
    "                epochs=30,\n",
    "                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Autoencoder output from corrupted test images\n",
    "x_decoded = autoencoder.predict(x_test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the 1st 8 corrupted and denoised images\n",
    "rows, cols = 10, 30\n",
    "num = rows * cols\n",
    "imgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]])\n",
    "imgs = imgs.reshape((rows * 3, cols, image_size, image_size))\n",
    "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
    "imgs = imgs.reshape((rows * 3, -1, image_size, image_size))\n",
    "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
    "imgs = (imgs * 255).astype(np.uint8)\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.title('Original images: top rows, '\n",
    "          'Corrupted Input: middle rows, '\n",
    "          'Denoised Input:  third rows')\n",
    "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
    "Image.fromarray(imgs).save('corrupted_and_denoised.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
